{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All). Or, alternatively, **Restart & Run All**.\n",
    "\n",
    "* Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\".\n",
    "\n",
    "* You can always add additional cells to the notebook to experiment, to test your answers, or to provide additional support for your answers.\n",
    "\n",
    "* You should not need to install new packages to complete an assignment. If you use any packages not available via the provided `Project.toml`, or if you change the `Manifest.toml`, then your assignment will likely not be graded correctly.\n",
    "\n",
    "* Submissions are only accepted via CANVAS!\n",
    "\n",
    "* Late submissions will not be accepted and receive 0%. There will be no exceptions.\n",
    "\n",
    "* By entering your name below you confirm that you have completed this assignment on your own. Suspicion of plagiarism / copying may result in an ad hoc oral examination on the content of the assignment. I may then downgrade an assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c66c5061c4c01ac1f864105cab029ef2",
     "grade": false,
     "grade_id": "cell-67ef8a6d9243c112",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# MATH 405/607 \n",
    "\n",
    "# Numerical Methods for Differential Equations\n",
    "\n",
    "## Assignment 2: Nonlinear systems, interpolation, quadrature\n",
    "\n",
    "\n",
    "#### Notes\n",
    "\n",
    "* This assignment will count for 10% of your total grade.\n",
    "\n",
    "* We will start to be rigorous about following instructions precisely to enable the autograder to correctly read your answers. Please watch out for those instructions. If a solution is correct but is not stored in the correct variables we will only give partial points. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e26efc59b78a88affdc02928d941d3c9",
     "grade": false,
     "grade_id": "cell-8e6d26a863226f59",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "include(\"math405.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "96e30b9899c1f34c807e6e6ed19be6eb",
     "grade": false,
     "grade_id": "cell-9f89f985bed8a924",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Question 1 [5+5]\n",
    "\n",
    "Skim the `README` files of [`Roots.jl`](https://github.com/JuliaMath/Roots.jl) and of [NLsolve.jl](https://github.com/JuliaNLSolvers/NLsolve.jl) to understand what these packages are written for. Then use appropriate functions provided by these packages to solve the following nonlinear equations: \n",
    "\n",
    "(a) Find all solutions $x \\in [0.1, 4.1\\pi]$ of \n",
    "$$\n",
    "    x^{-2} = \\sin(x)\n",
    "$$\n",
    "Assign these to a vector as follows: `Xa = [ x1, x2, ... ]`\n",
    "\n",
    "(b) Find the unique solution of the system  \n",
    "$$\\begin{aligned}\n",
    "    & f(x) := \\nabla \\varphi(x) = 0,  \\qquad \\text{where} \n",
    "    & \\varphi(x) = x_1^4 + \\sum_{j = 2}^{11} (x_j - x_{j-1})^4 + x_{11}^4 + 0.01 \\sum_{j = 1}^{11} x_j.\n",
    "\\end{aligned}$$\n",
    "and store it in the variable `xb` i.e. \n",
    "\n",
    "HINT: you need not derive and implement the gradient, but could use an AD package (e.g. `ForwardDiff.jl`) to obtain it from $\\varphi$, e.g., \n",
    "```julia \n",
    "phi(x) = ...\n",
    "fb(x) = ForwardDiff.gradient(phi, x)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3a417a0dda5fa2d9dc02a187e4cefdbb",
     "grade": false,
     "grade_id": "cell-44d680a2f35e30f5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Somewhere in this code you should have the lines \n",
    "#  Xa = [ ... ]\n",
    "#  xb =  ...\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a5e83d2563af860957470393b6eb57a1",
     "grade": true,
     "grade_id": "cell-5f260f440690ec8e",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2c11d3280745a6d93492d6136a9e6a29",
     "grade": false,
     "grade_id": "cell-e8d329407abed777",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Question 2 [10+5+10]\n",
    "\n",
    "Newton's method is chaotic (cf Fractals) and in general converges only locally, or at least its global behaviour is unpredictable for most practical purposes. Most implementations of Newton's method therefore have some \"globalisation\" strategies, i.e. incorporate ideas (heuristic and rigorous) to improve global convergence properties, or at least increase make the behaviour more predictable. In this question we will explore one strategy of this kind. \n",
    "\n",
    "Suppose your current iterate is $x_n$ then the next iterate would be $x_{n+1} = x_n - \\partial f(x_n)^{-1} f(x_n)$. But instead, let us define this increment as a *search vector*. That is, we define \n",
    "$$\n",
    "    p_n := - \\partial f(x_n)^{-1} f(x_n)\n",
    "$$\n",
    "and look for updates of the form \n",
    "$$\n",
    "    x_{n+1} = x_n + \\alpha_n p_n\n",
    "$$\n",
    "where $\\alpha_n \\in (0, 1]$.\n",
    "\n",
    "(a) Compute $\\frac{d}{d\\alpha} |f(x_n + \\alpha p_n)|^2$ at $\\alpha = 0$, then deduce that, for sufficiently small $\\alpha$ the update $x_{n+1}$ satisfies\n",
    "$$\n",
    "  \\text{(DEC)} \\qquad   |f(x_{n+1})| \\leq (1 - \\alpha_n/2) |f(x_n)|\n",
    "$$\n",
    "You may use any regularity on $f$ that you need.\n",
    "\n",
    "Remarks:\n",
    "* In your proof you should find that the factor $1/2$ in $(1-\\alpha/2)$ is arbitrary. But it is a sensible choice that seems to work quite well in the following tests.\n",
    "* You can proceed to part (b) without answering this question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0f1313ae1987b8aa22475092db0a5965",
     "grade": true,
     "grade_id": "cell-ffa1ea4278a8a5f3",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "11cab52b62bca64a2a63e3cbc2f084ff",
     "grade": false,
     "grade_id": "cell-e9a28558cdbe61f8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "(b) To achieve the (DEC) condition, we can use a backtracking algorithm. At each iterate $x_n$ our first guess should be $\\alpha = 1$ to obtain quadratic convergence in the limit. If this fails the (DEC) condition, then we halve $\\alpha$ until it is satisfied.\n",
    "``` \n",
    "WHILE ||f(x + alpha p)|| > (1-alpha/2) * ||f(x)||\n",
    "    alpha <- alpha / 2\n",
    "```\n",
    "* Implement this backtracking condition into a Newton iteration. \n",
    "* In light of part (a) of this question, terminate the backtracking with `return nothing` when $\\alpha < 10^{-8}$.\n",
    "\n",
    "In the code-cell below most of the two Newton methods have already been written. Only edit the part of the code that is indicated to incorporate the backtracking loop. Just translate the pseudocode into valid Julia code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b4e39385ae3016324af5194cfcac5861",
     "grade": false,
     "grade_id": "cell-36c1394e137ba5a0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "function newton(f, x0, tol, df = x -> ForwardDiff.jacobian(f, x); maxiter = 10)\n",
    "    x = x0 \n",
    "    it = 0\n",
    "    while norm(f(x)) > tol \n",
    "        x -= df(x) \\ f(x)\n",
    "        it += 1; \n",
    "        if (it > maxiter) || any(isnan, x) || any(isinf, x)\n",
    "            return nothing\n",
    "        end\n",
    "    end \n",
    "    return x, it \n",
    "end\n",
    "\n",
    "function damped_newton(f, x0, tol, df = x -> ForwardDiff.jacobian(f, x); maxiter = 100)\n",
    "    x = x0\n",
    "    it = 0\n",
    "\n",
    "    while norm(f(x), Inf) > tol \n",
    "        p = - (df(x) \\ f(x))\n",
    "        \n",
    "        # --------- Backtracking loop \n",
    "        # YOUR CODE HERE\n",
    "        # ---------------------------\n",
    "        \n",
    "        x += α * p\n",
    "        it += 1\n",
    "\n",
    "        # for debugging!\n",
    "        # @show α, norm(f(x), Inf)    \n",
    "\n",
    "        if it > maxiter; return nothing; end \n",
    "    end\n",
    "    return x, it \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell is not part of the assignment but is intended to help you \n",
    "# test that your code is correct. \n",
    "# you may edit this cell to experiment with your code ...\n",
    "\n",
    "println(\"\"\"\n",
    " This little test shows how Newton's method can converge \n",
    " predictably or to vastly different solution\n",
    " while the damped Newton method usually converges predictably.\n",
    "    \n",
    " Check that the damped Newton method always converges to the \n",
    " root near 6.38.\n",
    "\"\"\")\n",
    "\n",
    "# wrapping a scalar problem into a vectorial one (with 1 dimension of course...)\n",
    "fbes = x -> [besselj(3,x[1])]\n",
    "\n",
    "println(\"Newton - Start guess 4.85:\")\n",
    "xn1, itn1 = newton(fbes, [4.85], 1e-10)\n",
    "println(\"   x = $(xn1), #it = $(itn1)\")\n",
    "\n",
    "println(\"Newton - Start guess 4.84:\")\n",
    "xn2, itn2 = newton(fbes, [4.84], 1e-10)\n",
    "println(\"   x = $(xn2), #it = $(itn2)\")\n",
    "\n",
    "println(\"Damped Newton - Start guess 4.85:\")\n",
    "xd1, itd1 = damped_newton(fbes, [4.85], 1e-10)\n",
    "println(\"   x = $(xd1), #it = $(itd1)\")\n",
    "\n",
    "println(\"Damped Newton - Start guess 4.84:\")\n",
    "xd2, itd2 = damped_newton(fbes, [4.84], 1e-10)\n",
    "println(\"   x = $(xd2), #it = $(itd2)\")\n",
    "\n",
    "plot(x->besselj(3,x), 0, 20, lw=3, label=\"\", grid=:xy, size=(500,150), legend = :outertopright)\n",
    "scatter!([4.84], fbes(4.84), label = \"Starting guess\")\n",
    "scatter!(xn2, [0.0], label = \"Newton\")\n",
    "scatter!(xd2, [0.0], label = \"Damped Newton\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "09896393e7e9480f371f9f5e2856aae8",
     "grade": true,
     "grade_id": "cell-f4b7bc5a4255e555",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "961788f241d4a63fb583d2211999a251",
     "grade": false,
     "grade_id": "cell-07276558e3152b68",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "(c) Use both the Newton and damped Newton algorithms to solve the problem from Question 1, with starting guess `x0[i] = c * i * (12-i)` where `c in [-0.1, -0.01]` and briefly comment on your observations. Use comments such as ,\n",
    "```julia \n",
    "# we oberserve that `newton` ... while `damped_newton` ... \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f7ad46dce0d38a6d8f09e20ea5c0d6ec",
     "grade": true,
     "grade_id": "cell-6a8b70464005f257",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "20baa1cbb07b121063219767e175447d",
     "grade": false,
     "grade_id": "cell-ea22cf60859fca52",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Question 3: Simpson Rule [5+5+5+10]\n",
    "\n",
    "Consider the quadrature rule  (Simpson rule) \n",
    "$$ \n",
    "    \\int_{-h/2}^{h/2} f(x) \\,dx \\approx \\frac{h}{6} \\big( f(-h/2) + 4 f(0) + f(h/2) \\big)\n",
    "$$\n",
    "\n",
    "(a) We said that most quadrature rules can be interpreted as integrating a polynomial interpolant of the integrand. Which polynomial interpolant does the Simpson rule correspond to? (state without proof)\n",
    "\n",
    "(b) Derive an error bound, assuming that $f \\in C^4([a, b])$.  \n",
    "[Full points for a short proof that gets within a factor 2 of the sharp bound; partial points for a proof that gets the correct order.]\n",
    "\n",
    "(c) State (without proof) the corresponding estimate for the composite Simpson rule with mesh-size $h$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5e467b3aa87e7d28a46ffd32361f704a",
     "grade": true,
     "grade_id": "cell-201e3e52c4ddb022",
     "locked": false,
     "points": 15,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2be013ef9734d4c5381b6aad7e74a3c4",
     "grade": false,
     "grade_id": "cell-4f82a53a246a7fac",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "(d) Using the integral $\\int_0^\\pi x \\sin(x) \\,dx = \\pi$ as an example, numerically confirm the convergence rate predicted in (b): Produce a figure that compares the numerical convergence rate against the predicted rate. \n",
    "\n",
    "Ideally, you should first implement a function, e.g., `function simpson(f, a, b, N)` which implements the simpson rule. Then implement a short test (see lectures for inspiration!) which checks that the output of your function converges with the predicted rate to the analytic value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "82fb94749104abc54526906113e95ea0",
     "grade": true,
     "grade_id": "cell-5b4f7102a7e43c46",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# solution part (d) \n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5b9599fdb5a8dc099cbd2589453b0371",
     "grade": false,
     "grade_id": "cell-f0230a217b217ec9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 4 [5+5+5+5] : integrate some functions\n",
    "\n",
    "Integrate each of the following functions numerically. For each function you should \n",
    "* use an integrator that you implemented yourself (e.g., simpson from question 3?)\n",
    "* use the `Cubature.jl` package. \n",
    "Store the solutions in the variables `I_x` (your own method) and `Icub_x` (the `Cubature.jl` solution) where `x` is `a`, `b`, ...; e.g. \n",
    "```julia\n",
    "f_a = x -> exp(-x^2)\n",
    "I_a = my_method(f_a, ...) \n",
    "Icub_a, err_a = hquadrature(f_a, ...)\n",
    "```\n",
    "\n",
    "(a) $f_a(x) = e^{-x^2}, x \\in [0, 1]$\n",
    "\n",
    "(b) $f_b(x) = x \\log(x), x \\in [0, 1]$ \n",
    "\n",
    "(c) $f_c(x) = \\sqrt{x} \\log(x), x \\in [0, 1]$ \n",
    "\n",
    "(d) $f_d(x) = \\sqrt{x} \\exp(- 0.1 x), x \\in [1, \\infty]$\n",
    "\n",
    "I encourage you to use brute-force rather than get too clever about manually resolving the singularities. Let the computer do the work for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b967d30a83a62ceb73246ad9a839813c",
     "grade": false,
     "grade_id": "cell-ee6cff02f0804f93",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "using Cubature \n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "52fe04ea4dcc1579ec44a3822fa0532c",
     "grade": true,
     "grade_id": "cell-8980e79c277a9663",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.7.3",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
