{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MATH 405/607 \n",
    "\n",
    "# Numerical Methods for Differential Equations\n",
    "\n",
    "[[Instructor: Christoph Ortner]](http://www.math.ubc.ca/~ortner/)  [[course page]](https://github.com/cortner/math405_2022)\n",
    "\n",
    "# Guest Lecture: Polynomial Regression by Ruo Ning Qiu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Polynomial Regression \n",
    "\n",
    "* Linear Regression \n",
    "* Linear Least Square\n",
    "* Prediction Error\n",
    "\n",
    "### Literature \n",
    "\n",
    "* [Driscoll, Fundamentals of Numerical Computations, Linear Least Square](https://fncbook.github.io/fnc/leastsq/overview.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "include(\"math405.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting functions\n",
    "Suppose we are interested in an unknown function $f:\\R \\to \\R$. We have seen that we can approximate functions $f$ using polynomial interpolation by matching values of $f$ with the polynomials at the interpolation nodes. In many cases, interpolation is not the best way to fit a function. We can relax the requirement of matching values at some nodes by performing a polynomial expansion as a linear model to fit against $f$ instead.\n",
    "\n",
    "Define the linear model $\\tilde{f}$ to be\n",
    "\n",
    "$$ \\tilde{f}(x) = \\sum_{i=0}^N c_i P_i(x) = c^\\top P(x)$$\n",
    "\n",
    "where $c = (c_0, \\dots, c_N) \\in \\R^{N+1}$ is the undetermined parameter vector, $P(x) = (P_0(x), \\dots, P_N(x)) \\in \\R^{N+1}$ and $P_i(x)$ is a polynomial basis of degree $i$ such as the monomial polynomials of $P_i(x) = x^i$, and $N \\in \\mathbb{N}^+$ is the maximum degree of polynomial basis that the linear model has up to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Linear Least Square Problem\n",
    "Given a set of $M$ training data $\\{(x_j, y_j)\\}_{j=1, \\dots, M}$ where $y_j = f(x_j)$, we want to find the undetermined parameters $c^*$ in our linear model $\\tilde{f}$ that approximates $f$ well. One possible way is to minimize the difference square between the observed values and model fitting values at each training data $x_j$.\n",
    "\n",
    "$$c^* = \\argmin_{c \\in \\R^n} \\sum_{j=1}^M \\|y_j - \\tilde{f}(x_j)\\|^2$$\n",
    "\n",
    "This minimization/loss function can be written in a matrix multiplication form, similar to the interpolation as a linear system with Vandermonde matrix.\n",
    "\n",
    "$$ c^* = \\argmin_{c \\in \\R^n} \\| Y - Ac \\|^2 = \\argmin_{c \\in \\R^n} \\| Ac - Y \\|^2$$ \n",
    "where\n",
    "$$ A = \\begin{pmatrix}\n",
    "        P_1(x_1) & P_2(x_1) &  \\cdots & P_N(x_1)  \\\\ \n",
    "        P_1(x_2) & P_2(x_2) &  \\cdots & P_N(x_2)  \\\\ \n",
    "          \\vdots & \\vdots &        & \\vdots \\\\ \n",
    "        P_1(x_M) & P_2(x_M) &  \\cdots & P_N(x_M)  \\\\  \n",
    "        \\end{pmatrix}\n",
    "$$\n",
    "called the design matrix, and $Y = (y_1, \\dots, y_M) = (f(x_1), \\dots, f(x_M))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Normal Equation\n",
    "We can compute directly the minimizer of the loss function\n",
    "$$ \\phi(c) = \\| Ac - Y \\|^2$$\n",
    "as the minimizer must satisfy $\\nabla \\phi(c) = 0$, so\n",
    "$$ \\phi(c) = c^\\top A^\\top A c - 2 c^\\top A^\\top Y + \\|Y\\|^2$$\n",
    "we solve for $c^*$ with the following linear system\n",
    "$$\n",
    "\\begin{align}\n",
    "A^\\top A c^* &= A^\\top Y  \\\\ \n",
    "c^* &= (A^\\top A)^{-1} A^\\top Y\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QR Factorization\n",
    "We learned that the native way of solving linear system $Ax=b$ is `x = A \\ b` in Julia. However, there could be instability in solving the normal equation directly, since the condition number for normal equation is $\\kappa(A^\\top A) \\approx \\kappa(A)^2$ that can enlarge the instability. Check this [demo](https://fncbook.github.io/fnc/leastsq/demos/normaleqns-instab.html) for details. \n",
    "\n",
    "Thus, we want to perform a QR factorization of $A$ and substitute it into the normal equation, using the fact that every real $M \\times N$ matrix $A$  $(M \\leq N) $ can be written as $A = QR$, where $Q$ is an $M \\times M$ orthogonal matrix and $R$ is an $M \\times N$ upper triangular matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "A^\\top A c^* &= A^\\top Y  \\\\ \n",
    "R^\\top Q^\\top Q R c^* &= R^\\top Q^\\top Y \\\\\n",
    "R^\\top R c^* &=R^\\top Q^\\top Y \\\\\n",
    "R c^* &= Q^\\top Y \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "as $Q^\\top Q = I$, $R$ is non-singular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost of QR Factorization vs Solving the Normal Equation Directly\n",
    "We might wonder if the cost for QR factorization is really worth it to avoid the instability, i.e., how we balance between accuracy and computational cost. Let's compare it with the solving the normal equation with a LU factorization. Suppose $A$ is a $M \\times N, M \\leq N$ matrix.\n",
    "\n",
    "$$\\rm{COST}(qr(A)) = O(MN^2) $$\n",
    "vs \n",
    "$$\\rm{COST}(A^\\top A) + \\rm{COST}(lu(A)) =  O(MN^2 + N^3)$$\n",
    "\n",
    "<!-- 2 Scalar Multiple of qr than the other one  -->\n",
    "\n",
    "Read how to get the count of operations [here](http://www.math.iit.edu/~fass/477577_Chapter_5.pdf).\n",
    "\n",
    "It is really not more expensive to do a QR factorization, thus we should do it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example\n",
    "### Revisit: The Witch of Agnesi \n",
    "\n",
    "[some historical context](https://mathworld.wolfram.com/WitchofAgnesi.html)\n",
    "\n",
    "$$\n",
    "f(x) = \\frac{1}{1 + \\alpha^2 x^2}\n",
    "$$ \n",
    "has singularities at $m = \\pm i \\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define bases\n",
    "function poly(x::Number, N)\n",
    "    P = zeros(N)\n",
    "    for n = 0:(N-1)\n",
    "        P[n+1] = x^(n)\n",
    "    end\n",
    "    return P\n",
    "end\n",
    "\n",
    "function cheb(x::Number, N)\n",
    "    T = zeros(N)\n",
    "    T[1] = 1 \n",
    "    T[2] = x \n",
    "    for n = 2:(N-1) \n",
    "       T[n+1] = 2*x*T[n] - T[n-1]\n",
    "    end \n",
    "    return T \n",
    " end\n",
    "\n",
    " function design_matrix(X, basis, N)\n",
    "    A = zeros(ComplexF64, length(X), N)\n",
    "    for (i, x) in enumerate(X)\n",
    "      A[i, :] = basis(x, N)\n",
    "    end \n",
    "    return A\n",
    " end\n",
    "\n",
    " function predict(x::Float64, basis, c, N)\n",
    "    B = basis(x, N)\n",
    "    val = real(sum(c .* B))\n",
    "    return val\n",
    " end\n",
    " \n",
    " function predict(rr, args...)\n",
    "    vals = Float64[]\n",
    "    for r in rr\n",
    "       v = predict(r, args...)\n",
    "       push!(vals, v)\n",
    "    end\n",
    "    return vals\n",
    " end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "Random.seed!(405607)\n",
    "f = x -> 1/(1+25*x^2)\n",
    "\n",
    "M = 30 # sample size\n",
    "# generate train data\n",
    "Xtrain = rand(M) * 2 .- 1 \n",
    "# generate test data \n",
    "Xtest = range(-1, 1, length=300)\n",
    "\n",
    "basis = poly\n",
    "NN1 = [5, 8, 10, 20]\n",
    "\n",
    "xp = range(-1, 1, length=300)\n",
    "P1 = plot(xp, f.(xp); lw=3, label = \"exact\",\n",
    "          size = (400, 400), xlabel = L\"x\", ylabel = L\"f(x)\", ylim=[-2, 1.2], legend =:outerbottomright)\n",
    "Y = f.(Xtrain)\n",
    "plot!(P1, Xtrain, Y, lw=0, c = 1, m = :o, ms=3, label = \"train data\")\n",
    "\n",
    "for (iN, N) in enumerate(NN1)\n",
    "   A = design_matrix(Xtrain, basis, N)\n",
    "   Q,R = qr(A)\n",
    "   c = R \\ (Matrix(Q)'*Y)\n",
    "   plot!(P1, Xtest, predict(Xtest, basis, c, N), c = iN+1, lw=2, label = L\"p_{%$(N)}\")\n",
    "end \n",
    "display(P1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In comparison to the interpolation problem with equispaced or chebyshev nodes, we do not have a choice on the training data distribution in practice. Observe that for $p_{20}$, $30$ training data becomes not sufficient as we observe oscillations at the end of the function. \n",
    "\n",
    "Let's examine whether the error decreases if we keep increasing the sample size of the training data while fixing the degree of basis to be $20$. The error here is the root-mean-square error of\n",
    "$$\n",
    "RMSE = \\sum_{x \\in X_{\\rm test}} \\sqrt{\\frac{f(x) - \\tilde{f}(x)}{|X_{\\rm test}|}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(405607)\n",
    "N = 20\n",
    "MM = [ 40, 80, 160, 320, 640, 1000, 2000, 4000]\n",
    "\n",
    "bases = [ poly ]#, cheb ]\n",
    "\n",
    "err = zeros(length(bases), length(MM))   \n",
    "X = rand(20_000) * 2 .- 1        \n",
    "Xtest = sort(X[end:-1:end-10_000])\n",
    "Ytest = f.(Xtest)\n",
    "\n",
    "p1 = plot(xp, f.(xp); lw=1, label = \"exact\", c=0,\n",
    "          size = (400, 400), xlabel = L\"x\", ylabel = L\"f(x)\", ylim=[-2, 1.2], legend =:outerbottomright)\n",
    "\n",
    "for (iM, M) in enumerate(MM), (iB, basis) in enumerate(bases) \n",
    "   Xtrain = X[1:M]\n",
    "   Y = f.(Xtrain) \n",
    "   A = design_matrix(Xtrain, basis, N)\n",
    "   Q,R = qr(A)\n",
    "   c = R \\ (Matrix(Q)'*Y)\n",
    "   Ypred = predict(Xtest, basis, c, N)\n",
    "   plot!(p1, Xtest, Ypred, c = iM+1, lw=2, label = L\"M={%$(M)}\")\n",
    "   err[iB, iM] = norm(Ytest - Ypred, 2) / sqrt(length(Xtest))\n",
    "end\n",
    "\n",
    "p2 = plot(; legend=:topright, xlabel=\"M\", ylabel=\"rmse\", yscale=:log10, \n",
    "               xscale=:log10)\n",
    "plot!(p2, MM, err[1,:], m=:o, label=\"Mono Polynomial\", lw=3)\n",
    "# plot!(p2, MM, err[2,:], m=:o, label=\"Chebyshev Polynomial\", lw=2)\n",
    "plot(p1, p2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a degree of $20$ basis, the minimum root mean square error that the model converges to is around $10^{-2}$. \n",
    "\n",
    "What if we increases the degree of basis while feeding enough data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = [5, 7, 10, 13, 17, 21, 25, 30, 35, 40]\n",
    "MM = NN.^2 \n",
    "\n",
    "bases = [ poly ]#, cheb ]\n",
    "err = zeros(length(bases), length(NN))\n",
    "\n",
    "Xtrain = X[1:10_000]\n",
    "Xtest = X[end:-1:end-10_000]\n",
    "Ftest = f.(Xtest)\n",
    "\n",
    "for (iN, (N, M)) in enumerate(zip(NN, MM))\n",
    "   XM = X[1:M]\n",
    "   Y = f.(XM) \n",
    "   for (iB, basis) in enumerate(bases)\n",
    "      A = design_matrix(XM, basis, N)\n",
    "      Q,R = qr(A)\n",
    "      c = R \\ (Matrix(Q)'*Y)\n",
    "      err[iB, iN] = norm(Ftest - predict(Xtest, basis, c, N), 2) / sqrt(length(Xtest))\n",
    "   end\n",
    "end\n",
    "\n",
    "plt = plot(; legend=:topright, xlabel=\"N - degree of basis\", ylabel=\"RMSE\", yscale=:log10, title=\"Increase the degree of basis with sufficient train data\")\n",
    "plot!(plt, NN, err[1,:], m=:o, label=\"Mono Polynomial\", lw=2)\n",
    "# plot!(plt, NN, err[2,:], m=:o, label=\"Chebyshev Polynomial\", lw=1)\n",
    "# Plot error decay\n",
    "plot!(plt, NN, exp.( - asinh(1/25^(1/2)) * NN), ls=:dash, label=\"expected rate\")\n",
    "display(plt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "To control overfitting, we can add a regularization term to the loss function as\n",
    "$$ \\argmin_{c \\in \\R^n} \\| Ac - Y \\|^2 + \\lambda \\| c \\|^2$$\n",
    "where $\\lambda$ is a regularization coefficient that controls the relative strength of regularization. The regularization term here is the sum-of-squares of the parameter vector elements.\n",
    "\n",
    "There are various kinds of regularization terms. A more general regularizer is defined as\n",
    "$$\\lambda \\sum_{i=1}^N | c_i |^q$$\n",
    "where $q=2$ is the same as before, the quadratic regularizer. $q=1$ is known as the lasso regression that for sufficient large $\\lambda$, some of the $c_i$ become $0$ which leads to sparsity in the model that the corresponding basis functions $P_i$ play no role.\n",
    "\n",
    "In general, we use the quadratic regularizer in favour of the easy closed form solution that minimizes the loss function. Similar to A1 Q3b), the normal equation becomes\n",
    "\n",
    "$$\\begin{align}\n",
    "(A^\\top A + \\lambda I) c^* &= A^\\top Y  \\\\ \n",
    "c^* &= (A^\\top A + \\lambda I)^{-1} A^\\top Y\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(405607)\n",
    "f = x -> 1/(1+25*x^2)\n",
    "\n",
    "M = 30 # sample size\n",
    "λ = 0.1 # regularizer coefficient\n",
    "# generate train data\n",
    "Xtrain = rand(M) * 2 .- 1 \n",
    "# generate test data \n",
    "Xtest = range(-1, 1, length=300)\n",
    "\n",
    "basis = poly\n",
    "NN1 = [5, 8, 10, 20]\n",
    "\n",
    "xp = range(-1, 1, length=300)\n",
    "P1 = plot(xp, f.(xp); lw=3, label = \"exact\",\n",
    "          size = (400, 400), xlabel = L\"x\", ylabel = L\"f(x)\", ylim=[-2, 1.2], legend =:outerbottomright)\n",
    "plot!(P1, Xtrain, f.(Xtrain), lw=0, c = 1, m = :o, ms=3, label = \"train data\")\n",
    "\n",
    "for (iN, N) in enumerate(NN1)\n",
    "   Y = f.(Xtrain)\n",
    "   A = design_matrix(Xtrain, basis, N)\n",
    "   Reg = λ*I(N)\n",
    "   A = [A; Reg]\n",
    "   Y = [Y; zeros(N)]\n",
    "   Q,R = qr(A)\n",
    "   c = R \\ (Matrix(Q)'*Y)\n",
    "   plot!(P1, Xtest, predict(Xtest, basis, c, N), c = iN+1, lw=2, label = L\"p_{%$(N)}\")\n",
    "end \n",
    "\n",
    "display(P1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we are able to regularize the oscillations, we also pollute the solution if we pick $\\lambda=0.1$. Choose corresponding regularizers and bases based on the context of the problem that you want to solve. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary Polynomial Regression\n",
    "* approximation of general functions by using a linear model of polynomial bases to obtain the least square problem\n",
    "* the sample size of training data affects the error and could lead to problems like underfitting or overfitting\n",
    "* error convergence rate is related to the regularity of the target functions\n",
    "\n",
    "### Further reading\n",
    "Book: [Pattern Recognition and Machine Learning by Christopher Bishop](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf)\n",
    "* (Ch3.2) Different regularizers\n",
    "* (Ch3.3) Bayesian Ridge Regression\n",
    "* (Ch4.3.2) Non-linear Regression: Logistic, etc."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 1.8.0",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
