{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5f20197-6a8a-4ebd-abe3-456903fbfbbc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# MATH 405/607 \n",
    "\n",
    "# Method of Steepest Descent \n",
    "# (Also known as Gradient Descent)\n",
    "\n",
    "* Optimization problems & Root finding\n",
    "* Gradient Descent Scheme\n",
    "* Error Analysis\n",
    "* Global Convergence\n",
    "\n",
    "### Literature \n",
    "\n",
    "* [Boyd and Vandenberghe, Convex Optimization, Chapter 9](https://web.stanford.edu/~boyd/cvxbook/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "795cc739-e637-42f9-9e6b-32ca679c17bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: replacing module MATH405.\n"
     ]
    }
   ],
   "source": [
    "include(\"math405.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f424287f-c4ae-4d9e-9972-a3d37543a384",
   "metadata": {},
   "source": [
    "## Optimization Problems\n",
    "\n",
    "**Motivation:** Finding the minimum (\"optimum\") points of a function numerically. \n",
    "\n",
    "**Fundamental Principle:** By following the opposite direction of its gradient in each iteration.\n",
    "\n",
    "In other words, step towards where the function decreases the fastest. (\"Steepest\" descent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3464777-4d52-4fbe-858c-0b24ffa8029e",
   "metadata": {},
   "source": [
    "Simple example:\n",
    "\n",
    "Finding the minimum of\n",
    "$$f(x,y) = x^2 + y^2 $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d29679-27f3-49fa-a6c7-1af25960d686",
   "metadata": {},
   "source": [
    "![GradientDescenturl](https://blog.paperspace.com/content/images/2018/05/68747470733a2f2f707669676965722e6769746875622e696f2f6d656469612f696d672f70617274312f6772616469656e745f64657363656e742e676966.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d22e5c2-7a7a-43ee-b841-a2c9f3e1fc89",
   "metadata": {},
   "source": [
    "### Formulation\n",
    "Given a function $f \\in \\mathbb{R}^M$, which we need to find its minimum.\n",
    "\n",
    "Starting at some initial guess: $U_0 = \\begin{pmatrix} \n",
    "    x_{1, 0} \\\\ x_{2, 0} \\\\ \\vdots \\\\ x_{M, 0}\n",
    "    \\end{pmatrix}$\n",
    "\n",
    "The gradient of the function at this point is given by \n",
    "$$\\nabla f(U_n)= \n",
    "    \\begin{pmatrix} \n",
    "    \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_M}\n",
    "    \\end{pmatrix} \\, \\Bigg|_{\\,U_n}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac2b305-3cbc-4b7d-8b16-6e5c77f4a522",
   "metadata": {},
   "source": [
    "Then, we define the Gradient Descent Scheme:\n",
    "\n",
    "$$ U_{n+1} = U_n + h\\big( -\\nabla f(U_n)\\big)$$\n",
    "\n",
    "Where $h$ is the step size\n",
    "\n",
    "With the exit condition being $\\lVert \\nabla f(U_k) \\rVert \\leq \\epsilon$ for some small tolerance $\\epsilon$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab240f21-13b1-4278-8263-cea0785c0db7",
   "metadata": {},
   "source": [
    "### Advantage of the Gradient Descent Scheme:\n",
    "\n",
    "* Resilient to not so \"nice\" functions (Whose higher order derivatives does not exist or have unwanted zeros)\n",
    "* Moderate computation complexity (Requires only first order derivative)\n",
    "* Consistent convergence rate & Globally convergent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95768c7f-c4e7-4e4a-80c3-ea686ed05f08",
   "metadata": {},
   "source": [
    "### Comparison to Newton's Method in Optimization ï¼ˆ1-D Case)\n",
    "The Newton's Method of Optimization is defined as \n",
    "$$U_{n+1} = U_n - h\\frac{f'(U_n)}{f''(U_n)}$$\n",
    "(Finding the root of derivative instead of $f^0(x)$)\n",
    "\n",
    "Where the 1-D Gradient Descent Method is \n",
    "$$ U_{n+1} = U_n - hf'(U_n)$$\n",
    "\n",
    "Newton's Method requires $f''(U_n)$ to exist and is well-behaved (no nearby zeros)\n",
    "\n",
    "Where Gradient Descent only requires $f'(U_n)$ to exist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d82c08-febc-426a-bc20-63673d613b2b",
   "metadata": {},
   "source": [
    "### Example:\n",
    "Finding the minimum of $f(x) = cos(x)$ within $(0, \\pi)$ \n",
    "\n",
    "##### Newton's Method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a34651f-fee1-4bf7-bebf-c0412f861011",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = x -> cos(x)\n",
    "df = x -> ForwardDiff.derivative(f, x)\n",
    "d2f = x -> ForwardDiff.derivative(df, x)\n",
    "h = 0.01\n",
    "tolerance = 1e-8;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1507ff34-d43e-4e9a-8d9a-1cd103fe87db",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: plot not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: plot not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[1]:6",
      " [2] eval",
      "   @ ./boot.jl:373 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1196"
     ]
    }
   ],
   "source": [
    "#guess = pi/2+0.00001 # If you QA Engineer is having a bad day he's gonna do this\n",
    "#Believe or not, this ^^ is so ugly that even Jupyter Lab refused to save the notebook file (throws error 413)\n",
    "guess = pi/2 + 0.1\n",
    "\n",
    "x = guess; iter = [x]\n",
    "p = plot(f, 0.0, 2*pi, lw=3, grid=:xy, size=(800,300))\n",
    "scatter!([x], [f(x)], ms=4, color=:red)\n",
    "while(true)\n",
    "    old_x = x\n",
    "    x -= h * df(x) / d2f(x) ; push!(iter, x)\n",
    "    scatter!([x], [f(x)], ms=4, color=:red)\n",
    "    if abs(old_x - x) <= tolerance\n",
    "        break\n",
    "    end\n",
    "end\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042d1df0-2ce0-42f6-a41a-9a927f165d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "guess = pi/2+0.5\n",
    "\n",
    "x = guess; iter = [x]\n",
    "p = plot(f, 0.0, 2*pi, lw=3, grid=:xy, size=(800,300))\n",
    "scatter!([x], [f(x)], ms=4, color=:red)\n",
    "while(true)\n",
    "    old_x = x\n",
    "    x -= h * df(x) ; push!(iter, x)\n",
    "    scatter!([x], [f(x)], ms=4, color=:red)\n",
    "    if abs(old_x - x) <= tolerance\n",
    "        break\n",
    "    end\n",
    "end\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f949084-19ea-444d-9e4b-87986abf5f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Computational Cost:\n",
    "### L2-regularized Least Sqaures:\n",
    "For the L2-regularized least squares, running t iterations of gradient descent will take O(ndt), n examples and d features.\n",
    "The closed form solution: $w = (X^TX+\\lambda I)^{-1}(X^Ty)$, costs $O(nd^2 + d^3).\n",
    "\n",
    "So gradient descent is only faster if we only have to do $t <$ max$\\{d, d^2/n\\}$ iterations.\n",
    "\n",
    "### Logisitc Regression:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.3",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
