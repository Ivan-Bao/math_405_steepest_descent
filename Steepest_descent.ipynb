{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5f20197-6a8a-4ebd-abe3-456903fbfbbc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# MATH 405/607 \n",
    "\n",
    "# Method of Steepest Descent \n",
    "# (Also known as Gradient Descent)\n",
    "\n",
    "* Optimization problems & Root finding\n",
    "* Gradient Descent Scheme\n",
    "* Error Analysis\n",
    "* Global Convergence\n",
    "\n",
    "### Literature \n",
    "\n",
    "* [Boyd and Vandenberghe, Convex Optimization, Chapter 9](https://web.stanford.edu/~boyd/cvxbook/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795cc739-e637-42f9-9e6b-32ca679c17bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"math405.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f424287f-c4ae-4d9e-9972-a3d37543a384",
   "metadata": {},
   "source": [
    "## Optimization Problems\n",
    "\n",
    "**Motivation:** Finding the minimum (\"optimum\") points of a function numerically. \n",
    "\n",
    "**Fundamental Principle:** By following the opposite direction of its gradient in each iteration.\n",
    "\n",
    "In other words, step towards where the function decreases the fastest. (\"Steepest\" descent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3464777-4d52-4fbe-858c-0b24ffa8029e",
   "metadata": {},
   "source": [
    "Simple example:\n",
    "\n",
    "Finding the minimum of\n",
    "$$\n",
    "f(x,y) = x^2 + y^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d29679-27f3-49fa-a6c7-1af25960d686",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d22e5c2-7a7a-43ee-b841-a2c9f3e1fc89",
   "metadata": {},
   "source": [
    "### Formulation\n",
    "Given a function $f \\in \\mathbb{R}^M$, which we need to find its minimum.\n",
    "\n",
    "Starting at some initial guess: \n",
    "$$U_0 = \\begin{pmatrix} \n",
    "    x_{1, 0} \\\\ x_{2, 0} \\\\ \\vdots \\\\ x_{M, 0}\n",
    "    \\end{pmatrix}\n",
    "    $$\n",
    "\n",
    "The gradient of the function at this point is given by \n",
    "$$\n",
    "\\nabla f(U_n)= \n",
    "    \\begin{pmatrix} \n",
    "    \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_M}\n",
    "    \\end{pmatrix} \\, \\Bigg|_{\\,U_n}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac2b305-3cbc-4b7d-8b16-6e5c77f4a522",
   "metadata": {},
   "source": [
    "Then, we define the Gradient Descent Scheme:\n",
    "\n",
    "$$\n",
    "U_{n+1} = U_n + h\\big( -\\nabla f(U_n)\\big)\n",
    "$$\n",
    "\n",
    "Where $h$ is the step size\n",
    "\n",
    "With the exit condition being $\\lVert \\nabla f(U_k) \\rVert \\leq \\epsilon$ for some small tolerance $\\epsilon$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab240f21-13b1-4278-8263-cea0785c0db7",
   "metadata": {},
   "source": [
    "### Advantage of the Gradient Descent Scheme:\n",
    "\n",
    "* Resilient to not so \"nice\" functions (Whose higher order derivatives does not exist or have unwanted zeros)\n",
    "* Moderate computation complexity (Requires only first order derivative)\n",
    "* Consistent convergence rate & Globally convergent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcf3e50-d2d6-4c06-b369-2ad7309ec953",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95768c7f-c4e7-4e4a-80c3-ea686ed05f08",
   "metadata": {},
   "source": [
    "### Comparison to Newton's Method in Optimization ï¼ˆ1-D Case)\n",
    "The Newton's Method of Optimization is defined as \n",
    "$$\n",
    "U_{n+1} = U_n - h\\frac{f'(U_n)}{f''(U_n)}\n",
    "$$\n",
    "\n",
    "(Finding the root of derivative instead of $f^0(x)$)\n",
    "\n",
    "Where the 1-D Gradient Descent Method is \n",
    "$$\n",
    "U_{n+1} = U_n - hf'(U_n)\n",
    "$$\n",
    "\n",
    "Newton's Method requires $f''(U_n)$ to exist and is well-behaved (no nearby zeros)\n",
    "\n",
    "Where Gradient Descent only requires $f'(U_n)$ to exist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d82c08-febc-426a-bc20-63673d613b2b",
   "metadata": {},
   "source": [
    "### Example:\n",
    "Finding the minimum of\n",
    "$f(x) = cos(x)$ within $(0, \\pi)$ \n",
    "\n",
    "##### Newton's Method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a34651f-fee1-4bf7-bebf-c0412f861011",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = x -> cos(x)\n",
    "df = x -> ForwardDiff.derivative(f, x)\n",
    "d2f = x -> ForwardDiff.derivative(df, x)\n",
    "h = 0.03\n",
    "tolerance = 1e-8;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1507ff34-d43e-4e9a-8d9a-1cd103fe87db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#guess = pi/2+0.00001 # If you QA Engineer is having a bad day he's gonna do this\n",
    "#Believe or not, this ^^ is so ugly that even Jupyter Lab refused to save the notebook file (throws error 413)\n",
    "guess = pi/2 + 0.1\n",
    "\n",
    "x = guess; iter = [x]\n",
    "p = plot(f, 0.0, 2*pi, lw=3, grid=:xy, size=(800,300))\n",
    "scatter!([x], [f(x)], ms=4, color=:red)\n",
    "while(true)\n",
    "    old_x = x\n",
    "    x -= h * df(x) / d2f(x) ; push!(iter, x)\n",
    "    scatter!([x], [f(x)], ms=4, color=:red)\n",
    "    if abs(old_x - x) <= tolerance\n",
    "        break\n",
    "    end\n",
    "end\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042d1df0-2ce0-42f6-a41a-9a927f165d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "guess = pi/2+0.5\n",
    "\n",
    "x = guess; iter = [x]\n",
    "p = plot(f, 0.0, 2*pi, lw=3, grid=:xy, size=(800,300))\n",
    "scatter!([x], [f(x)], ms=6)\n",
    "while(true)\n",
    "    old_x = x\n",
    "    x -= h * df(x) ; push!(iter, x)\n",
    "    scatter!([x], [f(x)], ms=6)\n",
    "    if abs(old_x - x) <= tolerance\n",
    "        break\n",
    "    end\n",
    "end\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c5bb20-a1ff-4be6-8880-45dc5ea9b5fb",
   "metadata": {},
   "source": [
    "Test Cell adam is gonna put notes in\n",
    "\n",
    "General steepest descent methods:\n",
    "    move along the direction that leads to the steepest descent with respect to a certain parameter\n",
    "\n",
    "starting at some intial point $x_0$, we iterate $x_k$ by stepping: \n",
    "$$\n",
    "x_{k+1} = x_k + \\alpha_kd_k\n",
    "$$\n",
    "\n",
    "where $d_k$ is the direction of steepest decrease in the function $f(x)$ and:\n",
    "\n",
    "$$\n",
    "\\alpha_k = \\arg_{\\alpha} \\min f(x_k + \\alpha d_k)\n",
    "$$\n",
    "\n",
    "ex for finding the minima of a function $h(x,y)$:\n",
    "$$\n",
    "f(x,y) = h(x,y)\n",
    "$$\n",
    "ex for finding the solution to an equation $Ax = b$:\n",
    "$$\n",
    "f(x_k) = ||Ax_k-b||\n",
    "$$\n",
    "ex for \n",
    "\n",
    "**Gradient descent**:https://en.wikipedia.org/wiki/Gradient_descent  \n",
    "    descend along the negative gradient of a function to find a local minima.  \n",
    "        requires the function to be differentiable.  \n",
    "    inverse is gradient ascent (finding maxima)\n",
    "   \n",
    "**Coordinate descent**:https://en.wikipedia.org/wiki/Coordinate_descent  \n",
    "    iteratively move along coordinate directions to find a local minima of a function  \n",
    "        does not require differentiability (big plus)  \n",
    "\n",
    "**Conjugate gradient descent**:https://en.wikipedia.org/wiki/Conjugate_gradient_method  \n",
    "\n",
    "\n",
    "**Adaptive coordinate descent**:https://en.wikipedia.org/wiki/Adaptive_coordinate_descent  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7af190-6fe8-4917-9846-7fbdd4c8be5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.3",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
